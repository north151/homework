



# 1.决策树

```python
# 获取数据集
# 划分训练集,测试集
# 

# 选择最优划分属性
  特征取值离散
  特征取值连续:二分法
  return 最优划分属性,划分点
  

# 创建决策树
  1.训练集标签全相同 return 该标签值Y[i]
  2.训练集只有一列   return 出现次数最多标签值Y[i]
  3.最优划分属性,划分点
  4.递归构造子树:
    特征取值离散:构造子树(取值作划分标识)
    特征取值连续:构造左子树,构造右子树
    return 当前根节点

```



```python
# 生成决策树(数据点集 按树结构分类)
生成 Tree(D,K)

Tree(D,K)
1.创造根节点 root

2.if K ''                 
    root=null
    return root

3.K无值可选 or K全选完       # 无可划分属性
    root=null
    return root
    
4.从D中选最优划分属性K
  K=getc(D,K)
    
5.创建子节点
  for 分支k in 属性分支集K:   (D[k]==k)
        if D[k]为空:        # 该分支无数据点
        root->nodek = null
        else:               # 该分支有数据点
            root->nodek = Tree(D[k],K(大)-k(小))
            
5.输出决策树
  return root
            
        
        
  


```





```python
树结构  ==>  构建泛化能力强(处理未见样例能力高)决策树
根节点为一个测试属性  叶节点为决策结果

过程:  关键:如何选择最优划分属性
    1.创建节点;
      若该节点数据属于同一类,算法停止;
      该节点设为叶节点,用该类标记该节点(决策结果/标签)
    2.选择一个最好的分类属性作为测试属性
    3.对测试属性每种取值创建一个分支,以此划分样本
    4.递归构建树直至:
        所有样本同Y类    无可继续划分属性    无可继续划分数据
 

    




```





# 2.划分属性选择

## 1.信息增益(率)

```python
信息熵: 数据集类别分散程度
        每个节点中0与1的占比 - (P0*log2P0 + P1*log2P1)
信息增益: 划分前后信息熵之差
    
|Y|:打标标签类别数目     Pi:最终标签Y的分类占比(0与1的占比)
  D:当前节点数据集       Ak:按属性A划分第k分支划分数据集
  K:按属性A划分的分支数目 
 
选择信息增益最大/信息增益率最大的
```





$$
当前节点整体(直接按标签)信息熵:
\\Ent(D)=-\sum_{i=1}^{|Y|}P_{i}log_{2}P_i


\\按属性A(K种取值)划分后信息熵: \\Ent(D|A_k)=\sum_{i=1}^{|Y|}\frac{|A_ki|}{|A_k|}log_2\frac{|A_ki|}{|A_k|}
\\Ent(D|A)=\sum_{k=1}^{K}\frac{|A_k|}{|D|}Ent(D|A_k)
\\按属性A(K种取值)划分信息增益: 
\\Gain(D,A)=Ent(D)-Ent(D|A)
$$



$$
完美分类:将分支数作为打标种数,每分支为1种标签,求完美分类信息熵\\ IV(Xj)=-\sum_{k=1}^{K}\frac{|D^k_j|}{|D|}log2(\frac{|D^k_j|}{|D|})
$$

$$
按Xj划分信息增益率: GainRatio(D,Xj)=\frac{Gain(D,Xj)}{IV(Xj)}
$$





## 2.基尼指数

```
基尼指数: 数据集中随机抽两个样本点,标记不一样的概率
    基尼指数越小,纯度越高
|Y|:打标标记种数     pi:该标记占比
J:划分前分类属性数目  K:划分后分类属性数目
D:当前节点数据集      Djk:第k分支划分数据集(按第j属性划分)
```

$$
Gini(D)=1-\sum_{i=1}^{|Y|}p_i^2\\
$$


$$
按Xj划分后基尼指数: Gini(D,Xj)=\sum_{k=1}^KGini(D^k_j)\frac{|D^k_j|}{|D|}
$$




# 3.连续值处理

```
分类属性的值可能为连续值,需要进行离散化
分区间标记
```











