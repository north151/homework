



```python
# 分类
根据信息,将每一条记录分别属于哪一类标记出来

# 聚类
在没有训练的条件下把样本划分为若干类

```



# 1.聚类

```python
# 用于无监督学习
# m条样本(n维向量),划分为k个互不相交的簇(集合),由(λ1,...,λm)标记

# 簇内相似度高,簇间相似度低
```



## 1.性能度量

```python
# 评估聚类结果好坏    
```



### 外部指标

```python
# 以参考模型的输出作为标准，来评价聚类好坏
TP:a FP:b TN:c FN:d              
# [0,1]越大越好
```


$$
\begin{aligned}\\

& 定义:
\\
\\ & a=C\bigcap C`\qquad \qquad b=C-(C\bigcap C`)
\\ & c=C`-(C\bigcap C`)\qquad d=D-(C\bigcap C`)
\\

& Jaccard系数:
\\
& JC=\frac{a}{a+b+c}
		\qquad (\frac{TP}{TP+FP+TN})
\\

& FM指数:
\\
& FMI=\sqrt{\frac{a}{a+b}*\frac{a}{a+c}}
		\qquad (\sqrt{\frac{TP}{TP+FP}*\frac{TP}{TP+TN}})
\\

& Rand指数:
\\
& RI=\frac{2(a+d)}{m(m-1)}
		\qquad (\frac{TP+FN}{TP+FP+TN+FN})
\\


\end{aligned}
$$


​    

### 内部指标

```python
# 不依赖任何外部模型，直接对聚类的结果进行评估
# DBI越小越好,DI越大越好
```

  


$$
\begin{aligned}

\\
\\ & 备注:|C_i|:簇i中的样本数
\\


& 参数:
\\ 
& avg(C_i)=\frac
		{\sum_{1\leq i<j\leq|C_i|}dist(x_i,x_j)}
		{\frac
			{|C_i|*(|C_i|-1)}
			{2} 
		}
		\qquad(簇内部样本间平均距离=\frac{所有边之和}{边数})
\\

\\
& diam(C)=\max_{1\leq i<j\leq |c|}dist(xi,xj)
		\qquad(簇内部样本间最大距离)
\\

\\
& d_{min}(C_i,C_j)=\min_{xi\in Ci,xj\in Cj}dist(xi,xj)
		\qquad(C_i与C_j最近样本间距离)
\\

\\
& d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)
		\qquad(C_i与C_j中心间距离)
\\

& DB指数:
\\
& DBI=\frac{1}{K}
        \sum_{i=1}^{K}
        \max_{j\neq i}(
		\frac
			{avg(C_i)+avg(C_j)}
			{d_{cen}(\mu i,\mu j)}
		) 
		\qquad(DBI=平均\{\sum_{i=1}^{K} \max\frac{两簇内部平均距之和}{两簇中心距}\})
\\

& Dunn指数:
\\
& DI=\min_{1\leq i\leq K}\{
		\min_{j\neq i}(
		\frac
			{d_{min}(C_i,C_j)}
			{\max_{1\leq l\leq K} diam(C_l)}
			)
		\}
		
		
		\qquad(DI=最小\{\min_{j \neq i}(\frac{Ci与Cj最近样本距}{\max(簇内部样本间最大距离)})\}
\\

\end{aligned}
$$




## 2.统计距离

```python
基于 某种形式 的 "距离",距离越大,相似度越小
用于相似度度量的距离未必一定要满足度量的所有性质
例: A与B相似,B与C相似,A与C不相似


```



### 有序属性


$$
\begin{aligned}

\\ 
& 备注:X_{iu}:X_i的u属性(共n维)
\\


& 闵可夫斯基距离:
\\
& dist_{mk}(x_i,x_j)=\sqrt[p]{\sum_{u=1}^{n}{|x_{iu}-x_{ju}|}^{p}}
\\

& p=1,曼哈顿距离:
\\
& dist_{man}(x_i,x_j)={||x_i-x_j||_1}
                     =\sum_{u=1}^{n}{|x_{iu}-x_{ju}|^1}
\\

& p=2,欧式距离:
\\
& dist_{ed}(x_i,x_j)={||x_i-x_j||_2}
		        	=\sqrt{\sum_{u=1}^{n}|x_{iu}-x_{ju}|^2}
\\

\end{aligned}
$$




### 无序属性

$$
\begin{aligned}

\\ & 备注:
\\ & m_{u,a}:属性u上取a的样本数
\\ & m_{u,a,k}:簇C_k中属性u取a的样本数


\\
& VDM_{\rho}=\sum_{k=1}^{K}
	{|
		\frac
			{m_{u,a,k}}
		    {m_{u,a}} 
		 - 
		 \frac
		 	{m_{u,b,k}}
		 	{m_{u,b}}
	|}^{p}
\\

\end{aligned}
$$



### 混合属性

$$
\begin{aligned}


\\
& Minkov(x_i,x_j)=\sqrt[P]{
      \sum_{u=1}^{n_c}{|x_{iu}-x_{ju}|}^{p} +
      \sum_{u=n_c+1}^{n}{VDM_{\rho}(x_{iu}-x_{ju})}
      }
\\


& 加权距离:
\\
& dist_{wmk}(x_i,x_j)=\sqrt[P]{
              \sum_{u=1}^{n}{w_u{|x_{iu}-x_{ju}|}^{p}}
              }
\\

\end{aligned}
$$











# 2.算法

## 1.原型聚类

### 1.K均值

```python
# 构造KMeans类，封装k_mean算法的实现
class KMEANS:
    # 原始数据
    data = pd.DataFrame()
    # 算法分类标签
    tag="tag"
    # 中心点坐标列表
    centers_list=[]
    centers=np.zeros(shape=(1,1),dtype=float)
    # 距离函数
    dis=None
    
    #构造函数
    def __init__(self,data,label=None):
        #数据
        if label!=None:
            self.data=data.drop([label],axis=1)
        else:
            self.data=data
        self.data=PCA(n_components=2).fit_transform(self.data.values)
        #类别
        self.tag=pd.Series([0]*self.data.shape[0])
        #中心点
        self.centers=self.data[0].reshape(1,self.data[0].shape[0])
        self.centers_list=[]
        
         
    #距离计算
    def getDis(self,x,x0):
        qdis=(x-x0)**2
        if len(qdis.shape)!=1:
            qdis=np.sum(qdis,axis=1)
        else:
            qdis=np.sum(qdis)
        return qdis**0.5


    # 更新最佳中心点
    def update_center(self):
        data=self.data
        # 到中心的距离
        for i in range(data.shape[0]):
            dist = self.dis(self.centers,data[i])
            self.tag[i]=dist.argmin()
        # 更新簇中心坐标
        for i in range(self.centers.shape[0]):
            tmp = self.data[self.tag==i]
            num = tmp.shape[0]
            tmp = np.sum(tmp,axis=0)
            self.centers[i]=tmp/num

    #绘制
    def draw(self,tag):
        for i in set(tag):
            tmp=self.data[tag==i]
            plt.scatter(tmp[:,0],tmp[:,1])
        plt.show()
        
    
    # 运行算法
    def kmeans(self,num=1):
        #计算距离
        self.dis=self.getDis
        X=pd.DataFrame(self.data).drop_duplicates().reindex().values
        x=random.sample(range(X.shape[0]),num)
        self.centers = X[x]   
        pre=0.0
        for i in range(self.centers.shape[0]):
            tmp = self.data[self.tag==i]
            tmp=(tmp-self.centers[i])**2
            tmp=np.sum(tmp,axis=1)
            pre+=np.sum(tmp)

        while True:
            self.update_center()
            now=0.0
            for i in range(self.centers.shape[0]):
                tmp = self.data[self.tag==i]
                tmp=(tmp-self.centers[i])**2
                tmp=np.sum(tmp,axis=1)
                now+=np.sum(tmp)  
            if math.fabs(pre-now)<=0.1:
                break
            pre=now







```



### 1.Kmeans类

```python
# 利用实现的KMeans类，对Iris数据集进行聚类，并可视化聚类结果
def task_KMEANS(num=3):
    #获取数据
    Data=pd.read_csv('input_data/iris.csv',names=range(5))
    data=Data.copy()
    #创建对象
    k=KMEANS(data,4)
    k.kmeans(num)
    k.draw(k.tag)

task_KMEANS(5)

```









## 2.学习向量量化

### 1.LVQ类

```python
class LVQ:
    #两个样本点欧氏距离
    def euclidean_dist(self, x1, x2):
        x2=x2[0]
        dist = math.sqrt(math.pow(x1[0]-x2[0], 2)+math.pow(x1[1]-x2[1], 2))
        return dist

    #与样本最近的点
    def get_min_dist_proto_vect(self, proto_vects,x):
        min_dist = np.inf
        min_index = -1
        for i in range(len(proto_vects)):
            dist=self.euclidean_dist(proto_vects[i],x)
            if dist < min_dist:
                min_dist=dist
                min_index=i
        return min_index  

    #LVQ训练
    def fit(self,data_x,data_y,proto_vects,proto_labels,learnrate,max_iters):
        """
        参数：
        data_x:       训练集
        data_y:       标记
        proto_vects:  初始化的原型向量
        proto_labels: 原型向量对应的类别标记
        learnrate:    学习率
        max_iters:    最大迭代次数
        """
        
        iters = 0

    #LVQ训练
    def fit(self,data_x,data_y,proto_vects,proto_labels,learnrate,max_iters):
        """
        参数：
        data_x:       训练集
        data_y:       标记
        proto_vects:  初始化的原型向量
        proto_labels: 原型向量对应的类别标记
        learnrate:    学习率
        max_iters:    最大迭代次数
        """
        
        iters = 0
        while iters < max_iters:
            #随机选一个样本点
            sample_index = np.random.choice(len(data_x), 1, replace=False)
            #与其最近样本点
            min_dist_index = self.get_min_dist_proto_vect(proto_vects,data_x[sample_index])
            #根据样本的类别来更新原型向量
            if proto_labels[min_dist_index] == data_y[sample_index]:
                proto_vects[min_dist_index] = proto_vects[min_dist_index] + learnrate*(proto_vects[min_dist_index]-data_x[sample_index])
            else:
                proto_vects[min_dist_index] = proto_vects[min_dist_index] - learnrate*(proto_vects[min_dist_index]-data_x[sample_index])
            iters += 1
        return proto_vects
        

```



### 2.任务

```python

fig = plt.figure(1)
plt.subplot(221)
 #三个类簇的中心
center = [[1, 1], [-1, -1], [1, -1]]
cluster_std = 0.35
#生成一个具有三个类簇的数据集
X1, Y1 = make_blobs(n_samples=1000, centers=center,n_features=2, cluster_std=cluster_std, random_state=1)
#查看数据集
print(X1)
print(Y1[0:100])

lvq = LVQ()
#此处原型向量为随意设置
proto_vects=lvq.fit(X1,Y1,X1[0:100],Y1[0:100],0.01,60)

#数据集可视化
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
plt.scatter(proto_vects[:, 0],proto_vects[:, 1], marker='o', c=proto_labels)

```





## 2.密度聚类DBSCAN

```
类似于迷宫问题,是否可到达

```

### 1.DBSCAN类

```python
class DBSCAN:
    #计算两个向量的欧氏距离
    def euclidean_dist(self, x1, x2):
        x = np.array(x1)
        y = np.array(x2)
        return np.linalg.norm(x - y)
    
    #密度聚类
    def fit(self,data,neighbor_dist,minpts):
        """
        参数：
        data: 		  数据集
        neighbor_dist: 邻域半径
        minpts:        构成核心对象的邻域最小样本数
        """
            
        #核心对象集合
        core_obj = set()
        #聚类个数为0
        cluster_num = 0
        #聚类列表clusters
        clusters = []
        #未访问样本集合P
        P = set(data)
        #核心对象集合
        for sample in data:
            #如果样本sample的neighbor_dist邻域中样本数大于等于minpts
            if len([ sample_i for sample_i in data if self.euclidean_dist(sample, sample_i) <= neighbor_dist]) >= minpts:
                core_obj.add(sample)
                
                
        #开始聚类
        while len(core_obj): 
            #随机取一核心对象
            obj = list(core_obj)[np.random.randint(0, len(core_obj))]
            #访问该核心对象
            P = P - set(obj)
            
            #未访问集合
            P_OLD = P
            
            #初始化队列Q
            Q = []
            Q.append(obj)
            while len(Q):
                #队首
                q = Q[0]
                #q的邻域
                N_q = [sample for sample in data if self.euclidean_dist(sample, q) <= neighbor_dist]
                #q是核心对象
                if len(N_q) >= minpts:
                    #领域未访问对象入簇
                    delta = set(N_q) & P
                    Q += (list(delta))
                    P = P - delta 
                #出队
                Q.remove(q)
            #类簇+1
            cluster_num = cluster_num + 1
            #簇集合
            C_k = list(P_OLD - P)
            #剩余核心对象集合
            core_obj = core_obj - set(C_k)
            #记录簇
            clusters.append(C_k)
            
        return clusters




```



### 2.任务

```python
#简单任务
def task_DBSCAN():
    #数据集：每三个是一组分别是西瓜的编号，密度，含糖量
    data = [(0.697,0.460),(0.774,0.376),(0.634,0.264),(0.608,0.318),(0.556,0.215),
            (0.403,0.237),(0.481,0.149),(0.437,0.211),(0.666,0.091),(0.243,0.267),
            (0.245,0.057),(0.343,0.099),(0.639,0.161),(0.657,0.198),(0.360,0.370),
            (0.593,0.042),(0.719,0.103),(0.359,0.188),(0.339,0.241),(0.282,0.257),
            (0.748,0.232),(0.714,0.346),(0.483,0.312),(0.478,0.437),(0.525,0.369),
            (0.751,0.489),(0.532,0.472),(0.473,0.376),(0.725,0.445),(0.446,0.459)]
    d=DBSCAN()
    clusters=d.fit(data,0.11,5)
    draw(clusters)

'''
#task_DBSCAN()

'''


def test_DBSCAN(): 
    #获取数据
    Data = pd.read_excel(r"input_data/新闻.xlsx")
    data = Data.copy()
    #去除
    titel = data['标题'].map(lambda x:[i for i in list(jieba.cut(x)) if len(i) !=1])
    #词向量
    model = gensim.models.Word2Vec(titel,sg=0,vector_size=2,window=5,min_count=2,negative=3,sample=0.001,hs=1,workers=5)
    model.wv.save_word2vec_format('output_data/wordsX.txt',binary=False)
    model = gensim.models.KeyedVectors.load_word2vec_format('output_data/wordsX.txt',binary=False)
    
    #格式化数据集
    data_mat = [tuple(model[i]) for i in range(len(model))]
    #限制位数
    #data_mat = [tuple(round(xi,3) for xi in model[i]) for i in range(len(model))]      
    
    #DBSCAN
    d = DBSCAN()
    clusters = d.fit(data_mat,0.2,7)
    draw(clusters)

    #sklearn的DBSCAN
    sd = sklearn_DBSCAN(eps=0.2,min_samples=7)
    X = np.array(data_mat)
    Y = sd.fit_predict(X)
    plt.scatter(X[:,0],X[:,1],c=Y)
    plt.show()


'''
test_DBSCAN()

'''
```











## 3.层次聚类

```python
贪心算法,与Floyd有相似之处(不同处在选取点的方式)

# 1.把每个样本归为一类，计算每两个类之间的距离
# 2.寻找各个类之间最近的两个类,把他们归为一类(类数-1）
# 3.重新计算新生成的这个类与各个旧类之间的距离
# 4.重复2和3直到所有样本点都归为一类
```

### 1.AGNES类

```python
class AGNES:
    #欧氏距离
    def euclidean_dist(self, x1, x2):
        x = np.array(x1)
        y = np.array(x2)
        return np.linalg.norm(x - y)

    #两集合最小距离   
    def dist_min(self,C_i,C_j):
        min_dist = np.inf
        for x in C_i: 
            for z in C_j: 
                dist = self.euclidean_dist(x,z) 
                if dist < min_dist: 
                    min_dist = dist        

        return min_dist

    #两集合最大距离
    def dist_max(self,C_i,C_j):
        max_dist = -np.inf
        for x in C_i: 
            for z in C_j: 
                dist = self.euclidean_dist(x,z) 
                if dist > max_dist: 
                    max_dist = dist  

        return max_dist

    #两集合平均距离
    def dist_avg(self,C_i,C_j):
        avg_dist = 0.0
        for x in C_i: 
            for z in C_j: 
                avg_dist += self.euclidean_dist(x,z) 
        avg_dist = avg_dist / (len(C_i)*len(C_j)) 
        
        return avg_dist


    #最小距离及其下标
    def find_min_dist_pos(self,dist_matrix):
        min_dist =  np.inf
        x = 0
        y = 0
        for i in range(len(dist_matrix)):
            for j in range(len(dist_matrix[i])):
                if i!=j and dist_matrix[i][j] < min_dist:
                    min_dist = dist_matrix[i][j]
                    x = i
                    y = j
        return (x,y,min_dist)


    def cal_dist(self, dist_type,x,y):
        dist = 0.0
        if dist_type == 'min':
            dist = self.dist_min(x,y)
        elif dist_type == 'max':
            dist = self.dist_max(x,y)
        else:
            dist = self.dist_avg(x,y)
        return dist


    #层次聚类
    def fit(self,data,dist_type, k):
        """        
        参数：
        data: 数据集
        dist_type: 'min','max','avg'之一
        k: 聚类个数
        """
        #聚类划分
        clusters = []
        #初始簇
        for sample in data:
            C_i = []
            C_i.append(sample)
            clusters.append(C_i)
        #距离矩阵
        dist_matrix = []
        for x in clusters:
            m_i = []
            for y in clusters:
                dist = self.cal_dist(dist_type, x, y)
                m_i.append(dist)
            dist_matrix.append(m_i)
        
        #簇个数
        q = len(data)

        #合并,更新
        while q > k:
            x,y,min_dist = self.find_min_dist_pos(dist_matrix)
            #合并簇
            clusters[x].extend(clusters[y])
            clusters.remove(clusters[y])
            #更新距离矩阵
            dist_matrix = []
            for x in clusters:
                m_i = []
                for y in clusters:
                    dist = self.cal_dist(dist_type, x, y)
                    m_i.append(dist)
                dist_matrix.append(m_i)
            q = q - 1
        return clusters
    
    
```



### 2.任务

```python
#简单任务
def task_AGNES():
    #数据集：每三个是一组分别是西瓜的编号，密度，含糖量
    data = [(0.697,0.460),(0.774,0.376),(0.634,0.264),(0.608,0.318),(0.556,0.215),
            (0.403,0.237),(0.481,0.149),(0.437,0.211),(0.666,0.091),(0.243,0.267),
            (0.245,0.057),(0.343,0.099),(0.639,0.161),(0.657,0.198),(0.360,0.370),
            (0.593,0.042),(0.719,0.103),(0.359,0.188),(0.339,0.241),(0.282,0.257),
            (0.748,0.232),(0.714,0.346),(0.483,0.312),(0.478,0.437),(0.525,0.369),
            (0.751,0.489),(0.532,0.472),(0.473,0.376),(0.725,0.445),(0.446,0.459)]

    d=AGNES()
    clusters=d.fit(data,'avg',4)
    draw(clusters)

'''
task_AGNES()
'''


def test_AGNES(): 
    #获取数据
    Data = pd.read_excel(r"input_data/新闻.xlsx")
    data = Data.copy()
    #去除
    titel = data['标题'].map(lambda x:[i for i in list(jieba.cut(x)) if len(i) !=1])
    #词向量
    model = gensim.models.Word2Vec(titel,sg=0,vector_size=2,window=5,min_count=2,negative=3,sample=0.001,hs=1,workers=5)
    model.wv.save_word2vec_format('wordsX.txt',binary=False)
    model = gensim.models.KeyedVectors.load_word2vec_format('wordsX.txt',binary=False)
    
    #格式化数据集
    data_mat = [tuple(model[i]) for i in range(len(model))]   
    #限制位数
    #data_mat = [tuple(round(xi,3) for xi in model[i]) for i in range(len(model))]      
    #X = np.array(data_mat)

    #AGNES 算法
    a = AGNES()
    clusters = a.fit(data_mat, 'avg', 4)
    draw(clusters)

    #sklearn库的 AGNES 算法
    sa = sklearn_AGNES(n_clusters = 4)
    X = np.array(data_mat)
    Y = sa.fit_predict(X)
    plt.scatter(X[:,0], X[:,1],c=Y)
    plt.show()

test_AGNES()


```





## 4.其他

### 1.调用库

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import math
import gensim
from random import randint

import jieba
import jieba.posseg as pseg
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import DBSCAN as sklearn_DBSCAN
from sklearn.cluster import AgglomerativeClustering as sklearn_AGNES



```

### 2.分词,绘图

```python
#分词
def jieba_cut(titel):
    words = []
    segs = pseg.cut(titel)
    for word in segs:
        if word.flag in ['ns', 'n', 'vn', 'v', 'nr']:
            words.append(word.word)
    return words

#绘制聚类结果
def draw(C):
    colValue = ['r', 'y', 'g', 'b', 'c', 'k', 'm']
    for i in range(len(C)):
        coo_X = []    #x坐标列表
        coo_Y = []    #y坐标列表
        for j in range(len(C[i])):
            coo_X.append(C[i][j][0])
            coo_Y.append(C[i][j][1])
        plt.scatter(coo_X, coo_Y, marker='x', color=colValue[i%len(colValue)], label=i)
    plt.legend(loc='upper right')
    plt.show()

```











